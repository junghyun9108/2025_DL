{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junghyun9108/2025_DL/blob/main/20250421_CNN%EA%B3%BC%EC%A0%9C_%EC%A0%9C%EC%B6%9C_%EC%9D%B4%EC%A4%91%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ZXAYnVEvlcPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 데이터 전처리. 추가적인 전처리는 따로 정의하여 사용해주세요.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# CIFAR10 데이터셋 다운로드\n",
        "dataset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/20240513/', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# 학습 데이터셋과 검증 데이터셋으로 분할\n",
        "trainset, valset = random_split(dataset, [40000, 10000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64,\n",
        "                                        shuffle=False, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/20240513/', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n",
        "\n",
        "def calculate_accuracy(testloader, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))"
      ],
      "metadata": {
        "id": "qCQWRNF9VqRH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR10 분류 문제의 성능을 더욱 향상시키는 방법은 여러 가지가 있습니다:\n",
        "\n",
        "데이터 증강(Data Augmentation): 데이터 증강은 기존의 학습 데이터를 변형하여 새로운 학습 데이터를 생성하는 방법입니다. 이를 통해 모델이 다양한 변형에 대해 더욱 견고해질 수 있습니다. PyTorch의 torchvision.transforms 모듈을 사용하면 간단하게 데이터 증강을 적용할 수 있습니다.\n",
        "\n",
        "다른 모델 아키텍처 사용: 실습에 사용한 코드 외에도 다양한 CNN 아키텍처가 있습니다.ResNet, VGG, GoogLeNet, DenseNet 등 다른 모델 아키텍처를 사용할 수 있습니다.\n",
        "\n",
        "하이퍼파라미터 튜닝: 학습률, 배치 크기, 에폭 수 등의 하이퍼파라미터를 조정하여 성능을 향상시킬 수 있습니다.\n",
        "\n",
        "Early Stopping: 검증 세트의 성능이 더 이상 향상되지 않을 때 학습을 중단하는 기법입니다. 이를 통해 과적합을 방지할 수 있습니다.\n",
        "\n",
        "Regularization: L1, L2 정규화나 Dropout과 같은 정규화 기법을 사용하여 과적합을 방지할 수 있습니다.\n",
        "\n",
        "위 방법들을 사용하여,\n",
        "\n",
        "1) 실습 코드 마지막에 기재된 Accuracy of the network on the test images: 75.83 %보다 높은 성능을 갖는 코드를 작성해주세요.\n",
        "\n",
        "2) 그리고 본인이 작성한 코드가 왜 기존의 코드보다 좋은 성능이 나왔는지 서술해주세요."
      ],
      "metadata": {
        "id": "cNGd3STsXtwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 데이터 증강을 위한 변환 정의\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # RandomCrop 추가\n",
        "    transforms.RandomHorizontalFlip(),  # 랜덤하게 수평으로 뒤집기\n",
        "    transforms.RandomRotation(15),  # 랜덤하게 회전 (최대 15도)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# 테스트 데이터에는 데이터 증강을 적용하지 않음\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# CIFAR10 데이터셋 다운로드 및 로더 생성\n",
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/20240513/', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/20240513/', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "qX7tDzS-ckai"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# ResNet18 모델 로드 및 수정\n",
        "model = models.resnet18(pretrained=False).to(device)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10).to(device)  # CIFAR10에 맞게 출력층 수정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFSed9ccm4j",
        "outputId": "b87c7b45-5f08-4da9-b7ae-63debc42730f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)  # weight decay 추가\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)  # learning rate scheduler 추가\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "best_loss = None\n",
        "early_stop = False\n",
        "early_stop_count = 0\n",
        "\n",
        "# 모델 훈련\n",
        "epochs = 100\n",
        "for epoch in range(epochs):  # 데이터셋을 여러번 반복\n",
        "    if early_stop:\n",
        "        break\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # 200 미니배치마다 출력\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= i+1\n",
        "    print(f'Validation loss: {val_loss:.3f}')\n",
        "\n",
        "    scheduler.step()  # learning rate scheduler 적용\n",
        "\n",
        "    # early stopping 조건 확인\n",
        "    if best_loss is None or val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        early_stop_count = 0\n",
        "    else:\n",
        "        early_stop_count += 1\n",
        "        if early_stop_count >= patience:\n",
        "            early_stop = True\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8G7WGmTcpiG",
        "outputId": "0cb9b102-3e2a-42ae-cd13-c50f554a1cdf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 1.867\n",
            "[1,   400] loss: 1.620\n",
            "[1,   600] loss: 1.525\n",
            "Validation loss: 1.341\n",
            "[2,   200] loss: 1.377\n",
            "[2,   400] loss: 1.329\n",
            "[2,   600] loss: 1.301\n",
            "Validation loss: 1.101\n",
            "[3,   200] loss: 1.216\n",
            "[3,   400] loss: 1.164\n",
            "[3,   600] loss: 1.139\n",
            "Validation loss: 0.957\n",
            "[4,   200] loss: 1.100\n",
            "[4,   400] loss: 1.059\n",
            "[4,   600] loss: 1.062\n",
            "Validation loss: 0.899\n",
            "[5,   200] loss: 1.024\n",
            "[5,   400] loss: 0.997\n",
            "[5,   600] loss: 0.979\n",
            "Validation loss: 0.875\n",
            "[6,   200] loss: 0.956\n",
            "[6,   400] loss: 0.939\n",
            "[6,   600] loss: 0.927\n",
            "Validation loss: 0.770\n",
            "[7,   200] loss: 0.893\n",
            "[7,   400] loss: 0.887\n",
            "[7,   600] loss: 0.889\n",
            "Validation loss: 0.725\n",
            "[8,   200] loss: 0.871\n",
            "[8,   400] loss: 0.846\n",
            "[8,   600] loss: 0.861\n",
            "Validation loss: 0.678\n",
            "[9,   200] loss: 0.835\n",
            "[9,   400] loss: 0.831\n",
            "[9,   600] loss: 0.816\n",
            "Validation loss: 0.659\n",
            "[10,   200] loss: 0.797\n",
            "[10,   400] loss: 0.806\n",
            "[10,   600] loss: 0.790\n",
            "Validation loss: 0.631\n",
            "[11,   200] loss: 0.759\n",
            "[11,   400] loss: 0.782\n",
            "[11,   600] loss: 0.774\n",
            "Validation loss: 0.604\n",
            "[12,   200] loss: 0.764\n",
            "[12,   400] loss: 0.749\n",
            "[12,   600] loss: 0.755\n",
            "Validation loss: 0.558\n",
            "[13,   200] loss: 0.703\n",
            "[13,   400] loss: 0.735\n",
            "[13,   600] loss: 0.735\n",
            "Validation loss: 0.574\n",
            "[14,   200] loss: 0.718\n",
            "[14,   400] loss: 0.717\n",
            "[14,   600] loss: 0.718\n",
            "Validation loss: 0.581\n",
            "[15,   200] loss: 0.693\n",
            "[15,   400] loss: 0.678\n",
            "[15,   600] loss: 0.702\n",
            "Validation loss: 0.609\n",
            "[16,   200] loss: 0.685\n",
            "[16,   400] loss: 0.676\n",
            "[16,   600] loss: 0.685\n",
            "Validation loss: 0.525\n",
            "[17,   200] loss: 0.673\n",
            "[17,   400] loss: 0.685\n",
            "[17,   600] loss: 0.665\n",
            "Validation loss: 0.538\n",
            "[18,   200] loss: 0.658\n",
            "[18,   400] loss: 0.659\n",
            "[18,   600] loss: 0.669\n",
            "Validation loss: 0.515\n",
            "[19,   200] loss: 0.630\n",
            "[19,   400] loss: 0.646\n",
            "[19,   600] loss: 0.649\n",
            "Validation loss: 0.506\n",
            "[20,   200] loss: 0.634\n",
            "[20,   400] loss: 0.637\n",
            "[20,   600] loss: 0.634\n",
            "Validation loss: 0.490\n",
            "[21,   200] loss: 0.601\n",
            "[21,   400] loss: 0.617\n",
            "[21,   600] loss: 0.641\n",
            "Validation loss: 0.484\n",
            "[22,   200] loss: 0.607\n",
            "[22,   400] loss: 0.624\n",
            "[22,   600] loss: 0.614\n",
            "Validation loss: 0.441\n",
            "[23,   200] loss: 0.609\n",
            "[23,   400] loss: 0.600\n",
            "[23,   600] loss: 0.619\n",
            "Validation loss: 0.447\n",
            "[24,   200] loss: 0.596\n",
            "[24,   400] loss: 0.597\n",
            "[24,   600] loss: 0.615\n",
            "Validation loss: 0.434\n",
            "[25,   200] loss: 0.590\n",
            "[25,   400] loss: 0.588\n",
            "[25,   600] loss: 0.588\n",
            "Validation loss: 0.423\n",
            "[26,   200] loss: 0.575\n",
            "[26,   400] loss: 0.589\n",
            "[26,   600] loss: 0.597\n",
            "Validation loss: 0.441\n",
            "[27,   200] loss: 0.576\n",
            "[27,   400] loss: 0.595\n",
            "[27,   600] loss: 0.573\n",
            "Validation loss: 0.425\n",
            "[28,   200] loss: 0.577\n",
            "[28,   400] loss: 0.570\n",
            "[28,   600] loss: 0.565\n",
            "Validation loss: 0.441\n",
            "[29,   200] loss: 0.559\n",
            "[29,   400] loss: 0.559\n",
            "[29,   600] loss: 0.575\n",
            "Validation loss: 0.430\n",
            "[30,   200] loss: 0.548\n",
            "[30,   400] loss: 0.564\n",
            "[30,   600] loss: 0.565\n",
            "Validation loss: 0.418\n",
            "[31,   200] loss: 0.504\n",
            "[31,   400] loss: 0.478\n",
            "[31,   600] loss: 0.457\n",
            "Validation loss: 0.311\n",
            "[32,   200] loss: 0.452\n",
            "[32,   400] loss: 0.432\n",
            "[32,   600] loss: 0.433\n",
            "Validation loss: 0.298\n",
            "[33,   200] loss: 0.425\n",
            "[33,   400] loss: 0.419\n",
            "[33,   600] loss: 0.423\n",
            "Validation loss: 0.288\n",
            "[34,   200] loss: 0.409\n",
            "[34,   400] loss: 0.421\n",
            "[34,   600] loss: 0.418\n",
            "Validation loss: 0.284\n",
            "[35,   200] loss: 0.409\n",
            "[35,   400] loss: 0.395\n",
            "[35,   600] loss: 0.400\n",
            "Validation loss: 0.273\n",
            "[36,   200] loss: 0.406\n",
            "[36,   400] loss: 0.399\n",
            "[36,   600] loss: 0.385\n",
            "Validation loss: 0.268\n",
            "[37,   200] loss: 0.389\n",
            "[37,   400] loss: 0.398\n",
            "[37,   600] loss: 0.387\n",
            "Validation loss: 0.264\n",
            "[38,   200] loss: 0.392\n",
            "[38,   400] loss: 0.384\n",
            "[38,   600] loss: 0.375\n",
            "Validation loss: 0.260\n",
            "[39,   200] loss: 0.377\n",
            "[39,   400] loss: 0.373\n",
            "[39,   600] loss: 0.388\n",
            "Validation loss: 0.257\n",
            "[40,   200] loss: 0.380\n",
            "[40,   400] loss: 0.376\n",
            "[40,   600] loss: 0.379\n",
            "Validation loss: 0.249\n",
            "[41,   200] loss: 0.369\n",
            "[41,   400] loss: 0.376\n",
            "[41,   600] loss: 0.377\n",
            "Validation loss: 0.246\n",
            "[42,   200] loss: 0.354\n",
            "[42,   400] loss: 0.378\n",
            "[42,   600] loss: 0.373\n",
            "Validation loss: 0.242\n",
            "[43,   200] loss: 0.365\n",
            "[43,   400] loss: 0.360\n",
            "[43,   600] loss: 0.355\n",
            "Validation loss: 0.237\n",
            "[44,   200] loss: 0.352\n",
            "[44,   400] loss: 0.356\n",
            "[44,   600] loss: 0.370\n",
            "Validation loss: 0.236\n",
            "[45,   200] loss: 0.359\n",
            "[45,   400] loss: 0.357\n",
            "[45,   600] loss: 0.353\n",
            "Validation loss: 0.232\n",
            "[46,   200] loss: 0.343\n",
            "[46,   400] loss: 0.349\n",
            "[46,   600] loss: 0.359\n",
            "Validation loss: 0.228\n",
            "[47,   200] loss: 0.345\n",
            "[47,   400] loss: 0.349\n",
            "[47,   600] loss: 0.352\n",
            "Validation loss: 0.229\n",
            "[48,   200] loss: 0.348\n",
            "[48,   400] loss: 0.344\n",
            "[48,   600] loss: 0.346\n",
            "Validation loss: 0.226\n",
            "[49,   200] loss: 0.336\n",
            "[49,   400] loss: 0.338\n",
            "[49,   600] loss: 0.349\n",
            "Validation loss: 0.221\n",
            "[50,   200] loss: 0.341\n",
            "[50,   400] loss: 0.339\n",
            "[50,   600] loss: 0.343\n",
            "Validation loss: 0.219\n",
            "[51,   200] loss: 0.336\n",
            "[51,   400] loss: 0.329\n",
            "[51,   600] loss: 0.343\n",
            "Validation loss: 0.212\n",
            "[52,   200] loss: 0.324\n",
            "[52,   400] loss: 0.350\n",
            "[52,   600] loss: 0.347\n",
            "Validation loss: 0.215\n",
            "[53,   200] loss: 0.334\n",
            "[53,   400] loss: 0.335\n",
            "[53,   600] loss: 0.318\n",
            "Validation loss: 0.208\n",
            "[54,   200] loss: 0.321\n",
            "[54,   400] loss: 0.338\n",
            "[54,   600] loss: 0.322\n",
            "Validation loss: 0.203\n",
            "[55,   200] loss: 0.328\n",
            "[55,   400] loss: 0.318\n",
            "[55,   600] loss: 0.329\n",
            "Validation loss: 0.202\n",
            "[56,   200] loss: 0.322\n",
            "[56,   400] loss: 0.324\n",
            "[56,   600] loss: 0.317\n",
            "Validation loss: 0.199\n",
            "[57,   200] loss: 0.310\n",
            "[57,   400] loss: 0.312\n",
            "[57,   600] loss: 0.332\n",
            "Validation loss: 0.201\n",
            "[58,   200] loss: 0.314\n",
            "[58,   400] loss: 0.318\n",
            "[58,   600] loss: 0.321\n",
            "Validation loss: 0.192\n",
            "[59,   200] loss: 0.317\n",
            "[59,   400] loss: 0.314\n",
            "[59,   600] loss: 0.313\n",
            "Validation loss: 0.193\n",
            "[60,   200] loss: 0.313\n",
            "[60,   400] loss: 0.315\n",
            "[60,   600] loss: 0.310\n",
            "Validation loss: 0.189\n",
            "[61,   200] loss: 0.303\n",
            "[61,   400] loss: 0.294\n",
            "[61,   600] loss: 0.306\n",
            "Validation loss: 0.181\n",
            "[62,   200] loss: 0.297\n",
            "[62,   400] loss: 0.303\n",
            "[62,   600] loss: 0.296\n",
            "Validation loss: 0.181\n",
            "[63,   200] loss: 0.297\n",
            "[63,   400] loss: 0.301\n",
            "[63,   600] loss: 0.299\n",
            "Validation loss: 0.179\n",
            "[64,   200] loss: 0.294\n",
            "[64,   400] loss: 0.290\n",
            "[64,   600] loss: 0.303\n",
            "Validation loss: 0.178\n",
            "[65,   200] loss: 0.293\n",
            "[65,   400] loss: 0.298\n",
            "[65,   600] loss: 0.287\n",
            "Validation loss: 0.175\n",
            "[66,   200] loss: 0.284\n",
            "[66,   400] loss: 0.292\n",
            "[66,   600] loss: 0.279\n",
            "Validation loss: 0.177\n",
            "[67,   200] loss: 0.277\n",
            "[67,   400] loss: 0.276\n",
            "[67,   600] loss: 0.289\n",
            "Validation loss: 0.176\n",
            "[68,   200] loss: 0.281\n",
            "[68,   400] loss: 0.295\n",
            "[68,   600] loss: 0.286\n",
            "Validation loss: 0.175\n",
            "[69,   200] loss: 0.280\n",
            "[69,   400] loss: 0.289\n",
            "[69,   600] loss: 0.287\n",
            "Validation loss: 0.173\n",
            "[70,   200] loss: 0.288\n",
            "[70,   400] loss: 0.283\n",
            "[70,   600] loss: 0.287\n",
            "Validation loss: 0.173\n",
            "[71,   200] loss: 0.291\n",
            "[71,   400] loss: 0.295\n",
            "[71,   600] loss: 0.291\n",
            "Validation loss: 0.173\n",
            "[72,   200] loss: 0.289\n",
            "[72,   400] loss: 0.278\n",
            "[72,   600] loss: 0.284\n",
            "Validation loss: 0.172\n",
            "[73,   200] loss: 0.287\n",
            "[73,   400] loss: 0.284\n",
            "[73,   600] loss: 0.286\n",
            "Validation loss: 0.173\n",
            "[74,   200] loss: 0.292\n",
            "[74,   400] loss: 0.282\n",
            "[74,   600] loss: 0.283\n",
            "Validation loss: 0.172\n",
            "[75,   200] loss: 0.283\n",
            "[75,   400] loss: 0.282\n",
            "[75,   600] loss: 0.283\n",
            "Validation loss: 0.171\n",
            "[76,   200] loss: 0.272\n",
            "[76,   400] loss: 0.289\n",
            "[76,   600] loss: 0.285\n",
            "Validation loss: 0.171\n",
            "[77,   200] loss: 0.273\n",
            "[77,   400] loss: 0.276\n",
            "[77,   600] loss: 0.288\n",
            "Validation loss: 0.172\n",
            "[78,   200] loss: 0.281\n",
            "[78,   400] loss: 0.289\n",
            "[78,   600] loss: 0.282\n",
            "Validation loss: 0.171\n",
            "[79,   200] loss: 0.289\n",
            "[79,   400] loss: 0.284\n",
            "[79,   600] loss: 0.285\n",
            "Validation loss: 0.169\n",
            "[80,   200] loss: 0.275\n",
            "[80,   400] loss: 0.289\n",
            "[80,   600] loss: 0.274\n",
            "Validation loss: 0.169\n",
            "[81,   200] loss: 0.280\n",
            "[81,   400] loss: 0.283\n",
            "[81,   600] loss: 0.283\n",
            "Validation loss: 0.169\n",
            "[82,   200] loss: 0.281\n",
            "[82,   400] loss: 0.280\n",
            "[82,   600] loss: 0.286\n",
            "Validation loss: 0.167\n",
            "[83,   200] loss: 0.282\n",
            "[83,   400] loss: 0.282\n",
            "[83,   600] loss: 0.290\n",
            "Validation loss: 0.167\n",
            "[84,   200] loss: 0.284\n",
            "[84,   400] loss: 0.289\n",
            "[84,   600] loss: 0.278\n",
            "Validation loss: 0.166\n",
            "[85,   200] loss: 0.275\n",
            "[85,   400] loss: 0.277\n",
            "[85,   600] loss: 0.279\n",
            "Validation loss: 0.168\n",
            "[86,   200] loss: 0.281\n",
            "[86,   400] loss: 0.260\n",
            "[86,   600] loss: 0.276\n",
            "Validation loss: 0.166\n",
            "[87,   200] loss: 0.273\n",
            "[87,   400] loss: 0.277\n",
            "[87,   600] loss: 0.282\n",
            "Validation loss: 0.166\n",
            "[88,   200] loss: 0.271\n",
            "[88,   400] loss: 0.277\n",
            "[88,   600] loss: 0.284\n",
            "Validation loss: 0.166\n",
            "[89,   200] loss: 0.280\n",
            "[89,   400] loss: 0.285\n",
            "[89,   600] loss: 0.278\n",
            "Validation loss: 0.166\n",
            "[90,   200] loss: 0.277\n",
            "[90,   400] loss: 0.269\n",
            "[90,   600] loss: 0.272\n",
            "Validation loss: 0.166\n",
            "[91,   200] loss: 0.272\n",
            "[91,   400] loss: 0.269\n",
            "[91,   600] loss: 0.272\n",
            "Validation loss: 0.165\n",
            "[92,   200] loss: 0.283\n",
            "[92,   400] loss: 0.273\n",
            "[92,   600] loss: 0.273\n",
            "Validation loss: 0.165\n",
            "[93,   200] loss: 0.277\n",
            "[93,   400] loss: 0.288\n",
            "[93,   600] loss: 0.277\n",
            "Validation loss: 0.165\n",
            "[94,   200] loss: 0.284\n",
            "[94,   400] loss: 0.272\n",
            "[94,   600] loss: 0.275\n",
            "Validation loss: 0.165\n",
            "[95,   200] loss: 0.273\n",
            "[95,   400] loss: 0.275\n",
            "[95,   600] loss: 0.284\n",
            "Validation loss: 0.164\n",
            "[96,   200] loss: 0.277\n",
            "[96,   400] loss: 0.279\n",
            "[96,   600] loss: 0.279\n",
            "Validation loss: 0.164\n",
            "[97,   200] loss: 0.279\n",
            "[97,   400] loss: 0.285\n",
            "[97,   600] loss: 0.263\n",
            "Validation loss: 0.163\n",
            "[98,   200] loss: 0.278\n",
            "[98,   400] loss: 0.277\n",
            "[98,   600] loss: 0.275\n",
            "Validation loss: 0.164\n",
            "[99,   200] loss: 0.282\n",
            "[99,   400] loss: 0.279\n",
            "[99,   600] loss: 0.280\n",
            "Validation loss: 0.163\n",
            "[100,   200] loss: 0.281\n",
            "[100,   400] loss: 0.275\n",
            "[100,   600] loss: 0.288\n",
            "Validation loss: 0.164\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_accuracy(testloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxf0P3-ScrP3",
        "outputId": "e15f4243-962e-441f-b7e2-d8964e03e559"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 86.30 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능 향상 이유\n",
        "\n",
        "1) 데이터 증강: 데이터 증강을 통해 학습 데이터의 다양성을 높여 모델이 과적합되는 것을 방지하고, 다양한 변형에 대해 더욱 잘 학습되도록 구성하였습니다.\n",
        "\n",
        "2) ResNet18 모델: ResNet18은 CIFAR10 데이터셋에서 좋은 성능을 보이는 것으로 알려진 모델 아키텍처입니다. 잔차 연결(residual connection)을 사용하여 네트워크 학습 효율을 올렸습니다.\n",
        "\n",
        "3) 하이퍼파라미터 튜닝: weight decay, learning rate scheduler 등을 적용하여 모델의 학습 과정을 최적화하고, 더 나은 성능을 뽑을 수 있도록 수정하였습니다.\n",
        "\n",
        "4) Early Stopping: Early stopping을 통해 과적합을 방지하고, 검증 데이터셋에서 가장 좋은 성능을 보이는 모델을 선택하여 테스트 데이터셋에서도 좋은 성능을 얻을 수 있도록 작성하였습니다."
      ],
      "metadata": {
        "id": "e-Sc55bBovbV"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}